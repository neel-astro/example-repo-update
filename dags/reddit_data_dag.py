"""
Reddit_Data_Pipeline
DAG file auto-generated by AstroBuild.
"""
import time
from datetime import datetime

import pandas as pd
import praw
from airflow.decorators import dag, task
from airflow.models import Variable
from airflow.utils import timezone
from airflow.utils.dates import days_ago
from astro import sql as aql
from astro.files import File
from astro.sql.table import Table

reddit = praw.Reddit(
    client_id=Variable.get("REDDIT_CLIENT_ID"),
    client_secret=Variable.get("REDDIT_CLIENT_SECRET"),
    user_agent="Learn About AIrflow",
)


subreddits = [
    'dataengineering',
    'data',
    'datascience',
    'learnpython',
    'ETL', 
    'dataengineeringjobs',
    'BigDataJobs',
    'BigDataETL',
    'dataanalysis',
    'DataScienceJobs', 
    'MachineLearning'
]

@task(queue="python-tasks")
def find_data_platform_posts_func():
    keywords = [
        "snowflake",
        "databricks",
        "spark",
        "prefect",
        "dagster",
        "astronomer",
        "airflow",
        "mwaa",
        "composer",
        "fivetran"
    ]
    posts = []
    for keyword in keywords:
        for sub in subreddits:
            for post in reddit.subreddit(sub).new(limit=5000):
                if keyword in post.title.lower() or keyword in post.selftext.lower():
                    posts.append([post.title, post.score, post.id, post.url, post.num_comments, post.selftext, post.created, post.subreddit.display_name, keyword])
    
    return posts



@aql.dataframe()
def to_dataframe_func(posts: list):
    posts = pd.DataFrame(posts, columns=['title', 'score', 'id', 'url', 'num_comments', 'body', 'created', 'subreddit', 'keyword'])

    return posts

@aql.run_raw_sql(conn_id='david_snowflake')
def insert_into_snowflake_func(to_dataframe: Table):
    return """insert into REDDIT_DATA
    select * from {{to_dataframe}};
    """

@aql.run_raw_sql(conn_id='david_snowflake')
def delete_duplicate_posts_func():
    return """BEGIN
        create or replace transient table data_duplicate_holder as (
            SELECT 
                title,
                score,
                id,
                url,
                num_comments,
                body,
                created,
                subreddit,
                keyword,
                ROW_NUMBER() OVER (
                    PARTITION BY 
                        title, 
                        id
                    ORDER BY 
                        num_comments desc
                ) as row_num
            FROM 
                REDDIT_DATA
        );

        delete from data_duplicate_holder where row_num > 1;

        alter table data_duplicate_holder
        drop column row_num;

        -- time to use a transaction to insert and delete
        begin transaction;

        -- insert single copy
        alter table REDDIT_DATA SWAP WITH data_duplicate_holder;

        -- we are done
        commit;

        drop table data_duplicate_holder;
    END;
    """

@dag(schedule_interval="@daily", start_date=datetime(2022, 7, 27), catchup=False, tags=[])
def Reddit_Data_Pipeline():
    posts = find_data_platform_posts_func()
    to_dataframe = to_dataframe_func(posts)
    insert_into_snowflake_func(to_dataframe) >> delete_duplicate_posts_func() >> aql.cleanup()

dag_obj = Reddit_Data_Pipeline()